We'll structure implementation into 4 self-contained Codex Tasks:
| Step | Module        | Codex Task                                  |
| ---- | ------------- | ------------------------------------------- |
| 1Ô∏è‚É£  | `crawler.py`  | Extract all internal doc URLs from base URL |
| 2Ô∏è‚É£  | `renderer.py` | Render each URL to PDF (via Playwright)     |
| 3Ô∏è‚É£  | `merger.py`   | Merge all PDFs into one final PDF book      |
| 4Ô∏è‚É£  | `main.py`     | Orchestrate full pipeline via CLI           |

Let's proceed with the first one:
üíª Codex Task:  
`get_all_links()` function  

üìÅ Layer:  
crawler.py  

üéØ Objective:  
Extract all valid internal documentation page URLs from a given base URL.

üß© Specs:
- **Input:**  
  - `base_url: str` (e.g., "https://docs.telegram-mini-apps.com/")
- **Validation Rules:**  
  - Only follow links with the same domain
  - Skip non-HTML, external, anchor-only, and duplicate links
  - Respect `robots.txt` if present (optional for MVP)

- **Logic Flow:**  
  1. Start from `base_url`
  2. Use `requests.get()` to fetch HTML content
  3. Use `BeautifulSoup` to parse `<a href="">`
  4. Normalize each href with `urljoin`
  5. Deduplicate and filter to same domain
  6. Return a list of sorted URLs

- **Authorization:**  
  - Not required

- **Response Format:**  
  - `List[str]` of valid internal URLs

üß™ Tests:
- ‚úÖ Unit test with a local mock HTML file
- ‚úÖ Integration test against a test server with 3‚Äì5 pages
- ‚ùå Skip broken link retries for MVP

üì¶ Follow:  
- Pure function design: `get_all_links(base_url) -> list[str]`
- No logging or I/O in this layer ‚Äî that belongs in `logger.py` or `main.py`
